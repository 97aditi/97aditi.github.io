<!DOCTYPE html>
<html>
<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title> &#128640 LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro">
    <link rel="stylesheet" href="assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="assets/css/styles.css">
    <link rel="stylesheet" href="assets/css/Team-Clean.css">

<body>
    <div class="highlight-clean" style="padding-bottom: 10px; padding-top: 10px;">
            <h1 class="text-center">&#128640 LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms</h1>
        <div><p></p></div>
        <div class="container" style="max-width: 768px;background:white;">
            <div class="row">
                <div class="col-md-12", style="padding: 10px;">
                    <h4 class="text-center" style="margin: 0px;">
                        <a class="text-center", href = https://aditijha7.com>Aditi Jha<sup>1</sup></a>, 
                        <a class="text-center">Sam Havens<sup>2</sup></a>, 
                        <a class="text-center">Jeremey Dohman<sup>2</sup></a>, 
                        <a class="text-center">Alex Trott<sup>2</sup></a>, 
                        <a class="text-center">Jacob Portes<sup>2</sup></a>
                    </h4>
                    
                    <h5 class="text-center">Princeton University<sup>1</sup>, MosaicML x Databricks<sup>2</sup></h5> 
                    <h6 class="text-center">aditijha@princeton.edu, jacob.portes@databricks.com</h6> 
                </div>
            </div>
        </div> 
        <div class="buttons" style="margin-bottom: 8px;">
            <a class="btn btn-primary" role="button" href="https://github.com/97aditi/LIMIT-Less-is-more-for-instruction-tuning">GitHub</a>
            <a class="btn btn-primary" role="button" href="">Paper</a>
        </div> 
        <div><p></p></div>
        <!-- Figure -->
        <div><p></p></div>
        <div class="container" style="max-width: 768px;" >
            <div class="row">
                <div class="col-md-12 text-center"><img src="assets/LIMIT_intro_fig.png" 
                    style="width: 100%;margin-bottom: 8px;" alt="LIMIT schematic figure">
                </div>
                <em>How to finetune and evaluate LLMs for general purpose instruction following? (A) We finetune open-source LLMs MPT-7B and MPT-30B on datasets of varying sizes: Instruct-v1 and v3 which contain 56.2-59.3k instruction samples, and the LIMA dataset which contains 1,000 samples. (B) We then evaluate finetuned models using two paradigms: (1) traditional NLP perplexity-based evaluation on benchmarks such as MMLU and BIG-bench, as well as (2) model-based evaluation (via GPT-4) on open-ended generation.</em>
            </div>
        </div>
    </div>
    <script>
    </script>
    <style>
        body {
            font-family: 'Source Sans Pro', sans-serif;
            margin: 0;
            padding: 0;
            background: white;
        }
    
        .container {
            padding: 20px;
            margin: 0 auto;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
            background: white;
            margin-bottom: 30px;
        }

        .news-container {
            max-height: 200px; 
            overflow-y: auto; 
        }
    
        h1, h2, h3, h4, h5, h6 {
            color: #333;
        }
    
        /* Button Background */
        .btn-primary {
            background:blue;
            border-color: white;
            text-align: center;
        }
    
        .btn-primary:hover {
            background:blue;
            border-color: white;
            text-align: center;
        }
    
        /* Footer Gradient Background */
        .container:last-child {
            background: white; 
            color: white;
        }
    
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
    
        em {
            font-style: italic;
            color: black;
        }

        .api-appstore {
            margin-bottom: 20px;
        }
    </style>

<!-- Abstract -->
<div class="container" style="max-width: 768px;">
    <div class="row">
        <div class="col-md-12">
            <h2>Abstract</h2>
            <p style="text-align: justify;">
                Large Language Models are traditionally finetuned on large instruction datasets. However recent studies suggest that small, high-quality datasets can suffice for general purpose instruction following. This lack of consensus surrounding finetuning best practices is in part due to rapidly diverging approaches to LLM evaluation. In this study, we ask whether a small amount of diverse finetuning samples can improve performance on both traditional perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We finetune open-source MPT-7B and MPT-30B models on instruction finetuning datasets of various sizes ranging from 1k to 60k samples. We find that subsets of 1k-6k instruction finetuning samples are sufficient to achieve good performance on both (1) traditional NLP benchmarks and (2) model-based evaluation.
                Finally, we show that mixing textbook-style and open-ended QA finetuning datasets optimizes performance on both evaluation paradigms.
                <br>
            </p>
        </div>
    </div>
</div>

<!-- Result
<div class="container" style="max-width: 768px;background: #f0f7ff">
    <div class="row">
        <div class="col-md-12">
            <h2>Example</h2>
        </div>
        <div class="col-md-12 text-center"><img src="assets/img/results.png"
                style="width: 80%;margin-bottom: 8px;" alt="Gorilla LLM logo">
        </div>
        <em>Example API calls generated by GPT-4, Claude, and Gorilla for the
            given prompt. In this example, GPT-4 presents a model that doesnâ€™t exist, and Claude
            picks an incorrect library. In contrast, our model, Gorilla, can identify the task correctly
            and suggest a fully-qualified API call.</em>
    </div>
</div> -->

<!-- Citation -->
<div class="container" style="max-width: 768px">
    <div class="row">
        <div class="col-md-12">
            <h2>Citation</h2>
            <code>
                @article{jha2023limit,<br>
                    &nbsp;  title={LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms}, <br>
                    &nbsp;  author={Aditi Jha and Sam Havens and Jeremy Dohmann and Alex Trott and Jacob Portes},<br>
                    &nbsp;  journal={arXiv preprint },<br>
                    &nbsp;  year={2023},<br>
                }
            </code>
        </div>
    </div>
</div>

</body>

</html>